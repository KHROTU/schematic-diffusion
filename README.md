# schematic-diffusion

A 3D diffusion model trained to generate Minecraft schematics from natural language prompts. It uses a 3D U-Net architecture with cross-attention, conditioned on text embeddings from OpenAI's CLIP model.

## How to Train ~~Your Dragon~~ the Model

### Prerequisites

- A Windows or Linux machine with an NVIDIA GPU (8GB+ VRAM recommended).
- NVIDIA drivers and CUDA toolkit compatible with PyTorch.
- [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://repo.anaconda.com/archive/) installed (recommended).

### Step 1: Setup the Environment

First, clone the repository and set up the Conda environment.

```bash
# Clone this repository
git clone https://github.com/KHROTU/schematic-diffusion.git
cd schematic-diffusion

# Create and activate the Conda environment (recommended)
conda create --name schematic-diffusion python=3.10
conda activate schematic-diffusion

# Install PyTorch & CUDA
conda install pytorch torchvision toraudio pytorch-cuda=12.1 -c pytorch -c nvidia

# Install remaining dependencies (generated by pip, could contain unused packages)
pip install -r requirements.txt
```

When you first clone the repository, it should look like this:

```bash
+-- data
|   +-- 0_raw_downloads
|   +-- 2_named_schematics
|   +-- 3_litematics_to_convert
|   +-- 4_processed_tensors
|   L-- 1_id_to_name.txt
+-- litematic_converter
|   +-- converter.py
|   +-- converter_server.py
|   L-- Python Converter Bridge-1.0.user.js
+-- .gitignore
+-- 01a_triage_litematics.py
+-- 01_rename_files.py
+-- 02_generate_labels.py
+-- config.py
+-- generate.py
+-- preprocess_all_data.py
+-- README
+-- requirements.txt
L-- train_diffusion.py
```

When you finish training, it should look like this:

```bash
+-- data
|   +-- 0_raw_downloads/
|   +-- 2_named_schematics/
|   +-- 3_litematics_to_convert/
|   +-- 4_processed_tensors/
|   +-- 1_id_to_name.txt
|   L-- 5_labels.json
+-- litematic_converter
|   +-- converter.py
|   +-- converter_server.py
|   L-- Python Converter Bridge-1.0.user.js
+-- models
|   +-- schematic_diffusion_epoch_5.pth
|   +-- schematic_diffusion_epoch_10.pth
|   +-- ...
|   L-- schematic_diffusion_final.pth
+-- .gitignore
+-- 01a_triage_litematics.py
+-- 01_rename_files.py
+-- 02_generate_labels.py
+-- config.py
+-- generate.py
+-- preprocess_all_data.py
+-- README
+-- requirements.txt
L-- train_diffusion.py
```

### Step 2: Download the Dataset

The model was trained on a large dataset of schematics from the web. Due to the size of the dataset, it is not included in this repository.

1. **Download the Schematic Dataset:** Download the `0_raw_downloads.
zip` file containing ~120,000 raw schematic files.
    - **Link:** [MediaFire](https://www.mediafire.com/file/52ban1baf7vszgm/Schematics.zip/file) (thank you u/cbreauxgaming)
    - Unzip this file and place its contents into the `data/0_raw_downloads/` directory.

2. **Download the ID-to-Name Mapping:** This file maps the numeric filenames to their original names.
    - **Link:** [mclo.gs](https://mclo.gs/08RFHbg), [2](https://mclo.gs/QLpaA3w)
    - Place this file in the `data/` directory and name it `1_id_to_name.txt`.

For more context on the dataset (you might not want to use this specific one due ethical reasons), you can read the original [Reddit post](https://www.reddit.com/r/admincraft/comments/10ea2q6/almost_every_single_schematic_from/), specifically this [thread](https://www.reddit.com/r/admincraft/comments/10ea2q6/comment/j8oyx94/).

### Step 3: Preprocess the Data

Run the following scripts **in order**. Not sure what would happen if you run them out of order, but it is not recommended.

```bash
# Ensure your conda environment is active
conda activate schematic-diffusion

# 1. Rename files from IDs to human-readable names
python 01_rename_files.py

# 2. Separate .litematic files for conversion
python 01a_triage_litematics.py

# 3. Convert .litematic files to .schem (this will take a while)
# This requires the Tampermonkey script (litematic_converter\Python Converter Bridge-1.0.user.js)
# to be installed and active in your browser.
cd litematic_converter
python converter.py
cd ..

# 4. Generate the final labels.json file from the processed files
python 02_generate_labels.py

# 5. Convert all schematics into PyTorch tensors (this will also take a while, the output is ~80GB)
python preprocess_all_data.py
```

After this step, the `data/4_processed_tensors/` directory will be filled with your training-ready dataset.

### Step 4: Train the Model

```bash
# Start the training process
python train_diffusion.py
```

- The script will print the average loss after each epoch. You should see this value decrease over time.
- The training process is very long (100 epochs took ~14 hours on an RTX 4070 Laptop GPU), but you can speed it up by using a more powerful GPU with more VRAM, increasing the batch size, reducing the number of epochs, etc.
- Model checkpoints will be saved every 5 epochs to the `models/` directory.

### Step 5: Generate Schematics

Once the model is trained, you can generate new schematics using the generation script.

```bash
python generate.py
```

- You can modify the prompt and other parameters directly in the `generate.py` script.
